{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary package \n",
    "import yfinance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignoring warning messages\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  4 of 4 completed\n"
     ]
    }
   ],
   "source": [
    "# Using the .download() method to get our data\n",
    "\n",
    "raw_data = yfinance.download (tickers = \"^GSPC ^FTSE ^N225 ^GDAXI\", start = \"1994-01-07\", end = \"2023-10-11\", interval = \"1d\", group_by = 'ticker', auto_adjust = True)\n",
    "\n",
    "# tickers -> The time series we are interested in - (in our case, these are the S&P, FTSE, NIKKEI and DAX)\n",
    "# start -> The starting date of our data set\n",
    "# end -> The ending date of our data set (at the time of upload, this is the current date)\n",
    "# interval -> The distance in time between two recorded observations. Since we're using daily closing prices, we set it equal to \"1d\", which indicates 1 day. \n",
    "# group_by -> The way we want to group the scraped data. Usually we want it to be \"ticker\", so that we have all the information about a time series in 1 variable.\n",
    "# auto_adjust -> Automatically adjust the closing prices for each period. \n",
    "# treads - > Whether to use threads for mass downloading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=yfinance.download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a back up copy in case we remove/alter elements of the data by mistake\n",
    "df_comp = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns to the data set\n",
    "df_comp['spx'] = df_comp['^GSPC'].Close\n",
    "df_comp['dax'] = df_comp['^GDAXI'].Close\n",
    "df_comp['ftse'] = df_comp['^FTSE'].Close\n",
    "df_comp['nikkei'] = df_comp['^N225'].Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_comp = df_comp.iloc[1:] # Removing the first elements, since we always start 1 period before the first, due to time zone differences of closing prices\n",
    "del df_comp['^N225']  # Removing the original tickers of the data set\n",
    "del df_comp['^GSPC']\n",
    "del df_comp['^GDAXI']\n",
    "del df_comp['^FTSE']\n",
    "df_comp=df_comp.asfreq('b') # Setting the frequency of the data\n",
    "df_comp=df_comp.fillna(method='ffill') # Filling any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   spx          dax         ftse        nikkei\n",
      "                                                              \n",
      "Date                                                          \n",
      "1994-01-10  475.269989  2225.000000  3440.600098  18443.439453\n",
      "1994-01-11  474.130005  2228.100098  3413.800049  18485.250000\n",
      "1994-01-12  474.170013  2182.060059  3372.000000  18793.880859\n",
      "1994-01-13  472.470001  2142.370117  3360.000000  18577.259766\n",
      "1994-01-14  474.910004  2151.050049  3400.600098  18973.699219\n",
      "                    spx           dax         ftse        nikkei\n",
      "                                                                \n",
      "Date                                                            \n",
      "2023-10-04  4263.750000  15099.919922  7412.500000  30526.880859\n",
      "2023-10-05  4258.189941  15070.219727  7451.500000  31075.359375\n",
      "2023-10-06  4308.500000  15229.769531  7494.600098  30994.669922\n",
      "2023-10-09  4335.660156  15128.110352  7492.200195  30994.669922\n",
      "2023-10-10  4358.240234  15423.519531  7628.200195  31746.529297\n"
     ]
    }
   ],
   "source": [
    "print (df_comp.head()) # Displaying the first 5 elements to make sure the data was scraped correctly\n",
    "print (df_comp.tail()) # Making sure the last day we're including in the series are correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
